{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffcaa7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from IPython.display import Markdown, HTML\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087978dd",
   "metadata": {},
   "source": [
    "### Basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68b5b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 3340, which is longer than the specified 3000\n",
      "Created a chunk of size 3491, which is longer than the specified 3000\n",
      "Created a chunk of size 3408, which is longer than the specified 3000\n",
      "Created a chunk of size 3289, which is longer than the specified 3000\n",
      "Created a chunk of size 3172, which is longer than the specified 3000\n",
      "Created a chunk of size 3369, which is longer than the specified 3000\n",
      "Created a chunk of size 3253, which is longer than the specified 3000\n",
      "Created a chunk of size 3313, which is longer than the specified 3000\n",
      "Created a chunk of size 3200, which is longer than the specified 3000\n",
      "Created a chunk of size 3125, which is longer than the specified 3000\n",
      "Created a chunk of size 3331, which is longer than the specified 3000\n",
      "Created a chunk of size 3127, which is longer than the specified 3000\n",
      "Created a chunk of size 3570, which is longer than the specified 3000\n",
      "Created a chunk of size 3258, which is longer than the specified 3000\n",
      "Created a chunk of size 3425, which is longer than the specified 3000\n",
      "Created a chunk of size 3406, which is longer than the specified 3000\n",
      "Created a chunk of size 3060, which is longer than the specified 3000\n",
      "Created a chunk of size 3594, which is longer than the specified 3000\n",
      "Created a chunk of size 3177, which is longer than the specified 3000\n",
      "Created a chunk of size 3387, which is longer than the specified 3000\n",
      "Created a chunk of size 3493, which is longer than the specified 3000\n",
      "Created a chunk of size 3288, which is longer than the specified 3000\n",
      "Created a chunk of size 3077, which is longer than the specified 3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 3. Embed + 4. Build FAISS  \u001b[39;00m\n\u001b[32m     10\u001b[39m embeddings = OllamaEmbeddings(model = \u001b[33m'\u001b[39m\u001b[33mnomic-embed-text\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m faiss_index = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 5. Retrieval-QA chain  \u001b[39;00m\n\u001b[32m     14\u001b[39m qa = RetrievalQA.from_chain_type(\n\u001b[32m     15\u001b[39m     llm=ChatOllama(model = \u001b[33m\"\u001b[39m\u001b[33mgemma3:12b\u001b[39m\u001b[33m\"\u001b[39m), \n\u001b[32m     16\u001b[39m     chain_type=\u001b[33m\"\u001b[39m\u001b[33mstuff\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     17\u001b[39m     retriever=faiss_index.as_retriever(search_type=\u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m, search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m10\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfetch_k\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m50\u001b[39m})\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:848\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    846\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/langchain_community/embeddings/ollama.py:214\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[32m    206\u001b[39m \n\u001b[32m    207\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    211\u001b[39m \u001b[33;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m instruction_pairs = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.embed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/langchain_community/embeddings/ollama.py:202\u001b[39m, in \u001b[36mOllamaEmbeddings._embed\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    201\u001b[39m     iter_ = \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/langchain_community/embeddings/ollama.py:167\u001b[39m, in \u001b[36mOllamaEmbeddings._process_emb_response\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    161\u001b[39m headers = {\n\u001b[32m    162\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    163\u001b[39m     **(\u001b[38;5;28mself\u001b[39m.headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    164\u001b[39m }\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     res = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/api/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. Load docs\n",
    "loader = TextLoader(\"docs/StockWatson.txt\")  # or PyPDFLoader for PDFs\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Chunk\n",
    "splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=800)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 3. Embed + 4. Build FAISS  \n",
    "embeddings = OllamaEmbeddings(model = 'nomic-embed-text')\n",
    "faiss_index = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# 5. Retrieval-QA chain  \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOllama(model = \"gemma3:12b\"), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=faiss_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10, \"fetch_k\": 50})\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fec46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "answer = qa.invoke(\"What are the assumptions of Linear Regression and how to test them?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e763cd6b",
   "metadata": {},
   "source": [
    "## Better Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c5ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4be76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "020eb311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_430302/4238296662.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model = 'nomic-embed-text')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Load and enhanced split\n",
    "loader = TextLoader(\"docs/StockWatson.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1800, chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 3. FAISS with metadata\n",
    "embeddings = OllamaEmbeddings(model = 'nomic-embed-text')\n",
    "\n",
    "faiss_index = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# 4. Metadata-aware retriever\n",
    "retriever = faiss_index.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":5, 'top_k': 20},\n",
    ")\n",
    "\n",
    "# 5. Retrieval-QA chain  \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOllama(model = \"gemma3:12b\"), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80cf2d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down the assumptions of linear regression and how to test them. The assumptions are critical because they underpin the validity of the statistical inferences (hypothesis tests, confidence intervals) you draw from your regression model. Violations can lead to incorrect conclusions.\n",
       "\n",
       "**Key Concepts First: The Linear Regression Model**\n",
       "\n",
       "Before diving into assumptions, let's quickly recap the basic linear regression model:\n",
       "\n",
       "Y<sub>i</sub> = β<sub>0</sub> + β<sub>1</sub>X<sub>1i</sub> + β<sub>2</sub>X<sub>2i</sub> + ... + β<sub>k</sub>X<sub>ki</sub> + ε<sub>i</sub>\n",
       "\n",
       "Where:\n",
       "\n",
       "*   Y<sub>i</sub>: The dependent variable for observation i\n",
       "*   X<sub>1i</sub>, X<sub>2i</sub>, ..., X<sub>ki</sub>: The independent (predictor) variables for observation i\n",
       "*   β<sub>0</sub>: The intercept (constant term)\n",
       "*   β<sub>1</sub>, β<sub>2</sub>, ..., β<sub>k</sub>:  The coefficients (slopes) representing the effect of each independent variable on the dependent variable.\n",
       "*   ε<sub>i</sub>: The error term (residual) for observation i, representing the difference between the actual value of Y and the value predicted by the model.\n",
       "\n",
       "**The Assumptions (and How to Test Them)**\n",
       "\n",
       "Here’s a detailed breakdown of the standard linear regression assumptions, their significance, and methods to test them.  I've divided them into \"Essential\" and \"Technical\" because the former are absolutely critical for the validity of most standard inference, and the latter are more technical and often relate to specific properties of estimators.\n",
       "\n",
       "**I. Essential Assumptions (Critical for Inference)**\n",
       "\n",
       "1.  **Linearity:**\n",
       "    *   **What it means:**  The relationship between the independent variables and the dependent variable is linear. This doesn’t mean that the relationship is a straight line for *every* single observation. It means that the *average* or expected value of Y changes linearly with changes in the independent variables.\n",
       "    *   **How to test:**\n",
       "        *   **Scatterplots:** Create scatterplots of each independent variable against the dependent variable. Look for non-linear patterns (curves, exponential growth, etc.).\n",
       "        *   **Residual Plots:**  Plot the residuals (the difference between the actual and predicted values) against each independent variable.  A random scatter of points around zero suggests linearity.  Patterns (e.g., a curved shape) indicate non-linearity.\n",
       "        *   **Transformations:** If non-linearity is suspected, try transforming variables (e.g., taking logarithms, squares, cube roots).\n",
       "2.  **Independence of Errors:**\n",
       "    *   **What it means:** The error terms for different observations are independent of each other. In other words, the error for one observation doesn't influence the error for another.  This is *especially* critical for time-series data.\n",
       "    *   **How to test:**\n",
       "        *   **Durbin-Watson Test:** A formal test for autocorrelation (correlation between error terms).  A value close to 2 suggests no autocorrelation. Values significantly below or above 2 suggest autocorrelation. (Note: the interpretation of the Durbin-Watson statistic can be complex and is highly dependent on the characteristics of the data and the regression model.)\n",
       "        *   **Residual Autocorrelation Plot:** Plot the residuals against their order in the data. A pattern here suggests autocorrelation.\n",
       "        *   **Time Series Plots:** If you have time series data, plot the actual values of Y and the residuals over time. Look for patterns.\n",
       "3.  **Homoskedasticity (Constant Variance of Errors):**\n",
       "    *   **What it means:** The variance of the error term is constant for all values of the independent variables.  \"Heteroskedasticity\" means that the variance is *not* constant – it changes with the independent variables.\n",
       "    *   **How to test:**\n",
       "        *   **Residual Plots:** Plot the residuals against the predicted values or against independent variables.  Look for a \"funnel shape\" or other patterns that suggest the variance is changing.\n",
       "        *   **Breusch-Pagan Test or White Test:** Formal statistical tests for heteroskedasticity.  These tests have null hypotheses of homoskedasticity.\n",
       "4.  **Normality of Errors:**\n",
       "    *   **What it means:** The error terms are normally distributed. This assumption is less critical for large sample sizes (due to the Central Limit Theorem), but it’s important for small samples or when making precise inferences.\n",
       "    *   **How to test:**\n",
       "        *   **Histograms or Density Plots of Residuals:** Visually check if the residuals resemble a normal distribution.\n",
       "        *   **Q-Q Plot (Quantile-Quantile Plot):**  Plots the quantiles of the residuals against the quantiles of a normal distribution. If the residuals are normally distributed, the points will fall close to a straight line.\n",
       "        *   **Jarque-Bera Test:** A formal test for normality.\n",
       "\n",
       "**II. Technical Assumptions (Affect Efficiency & Properties)**\n",
       "\n",
       "1.  **Zero Mean of Errors:**\n",
       "    *   **What it means:**  The expected value of the error term is zero.  This is usually met if the intercept is appropriately estimated.\n",
       "    *   **How to test:**  This is typically checked implicitly through the regression process itself. A poorly estimated intercept can suggest this assumption is violated.\n",
       "2.  **No Multicollinearity:**\n",
       "    *   **What it means:**  The independent variables are not highly correlated with each other. High multicollinearity can inflate standard errors, making it difficult to determine the individual effects of the variables.\n",
       "    *   **How to test:**\n",
       "        *   **Correlation Matrix:** Calculate the correlation coefficients between all pairs of independent variables.  High correlations (e.g., > 0.7 or 0.8) suggest multicollinearity.\n",
       "        *   **Variance Inflation Factor (VIF):** A measure of how much the variance of an estimated coefficient is inflated due to multicollinearity.  A VIF greater than 5 or 10 often indicates a problem.\n",
       "\n",
       "**Important Considerations:**\n",
       "\n",
       "*   **Remedial Measures:** If assumptions are violated, you can take steps to address the problems, such as transforming variables, adding new variables, or using different estimation techniques.\n",
       "*   **Severity of Violations:** Not all violations are equally serious. Some violations (e.g., mild non-normality in a large sample) may have little impact on inferences, while others (e.g., severe heteroskedasticity) can lead to incorrect conclusions.\n",
       "*   **Software:**  Most statistical software packages (e.g., R, Stata, SPSS, Python with libraries like statsmodels) provide tools to test these assumptions and diagnose potential problems.\n",
       "\n",
       "\n",
       "\n",
       "Let me know if you'd like a deeper dive into any specific assumption or testing method!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example query\n",
    "answer = qa.invoke(\"What are the assumptions of Linear Regression and how to test them? Explain each point in detail.\")\n",
    "Markdown(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a3c15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import HuggingFaceRerank\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "# 1. Load the source document(s) from disk.\n",
    "loader = TextLoader(\"docs/StockWatson.txt\")\n",
    "# TextLoader reads a plain-text file and wraps it as a LangChain Document.\n",
    "docs = loader.load()\n",
    "# load() returns a list of Document objects containing the file text.\n",
    "\n",
    "\n",
    "# 2. Split Documents into overlapping chunks.\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1800,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "# We split on paragraph breaks first, then lines, then spaces, falling back to characters.\n",
    "chunks = splitter.split_documents(docs)\n",
    "# This yields many smaller Document chunks, which improves retrieval precision and LLM context handling.\n",
    "\n",
    "\n",
    "# 3. Build two retrievers: BM25 for keyword matches…\n",
    "bm25 = BM25Retriever.from_documents(chunks)\n",
    "# …and FAISS for semantic embedding search.\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "faiss_index = FAISS.from_documents(chunks, embeddings)\n",
    "faiss_ret = faiss_index.as_retriever()\n",
    "\n",
    "\n",
    "# 4. Combine BM25 + FAISS using Reciprocal Rank Fusion.\n",
    "ensemble = EnsembleRetriever(retrievers=[bm25, faiss_ret])\n",
    "# EnsembleRetriever merges rankings from multiple retrievers to balance exact-match and semantic recall.\n",
    "\n",
    "\n",
    "# 5. Set up a Hugging Face cross-encoder reranker.\n",
    "reranker = HuggingFaceRerank(\n",
    "    model_name_or_path=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    # This lightweight cross-encoder scores each (query, passage) pair.\n",
    "    top_k=10\n",
    "    # We’ll rerank only the top 10 candidates from the ensemble.\n",
    ")\n",
    "# HuggingFaceRerank uses the specified model to assign fine-grained relevance scores.\n",
    "\n",
    "\n",
    "# 6. Wrap retrieval + reranking in a ContextualCompressionRetriever.\n",
    "cc_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=ensemble,\n",
    "    base_compressor=reranker\n",
    ")\n",
    "# On each query, LangChain will:\n",
    "#   a) call ensemble.get_relevant_documents(query) → candidates\n",
    "#   b) call reranker.rerank(query, candidates) → reranked, trimmed list\n",
    "#   c) return that final list into the QA chain.\n",
    "\n",
    "\n",
    "# 7. Assemble the final RetrievalQA chain with “stuff” combiner.\n",
    "qa_hf_rerank = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOllama(model=\"gemma3:12b\"),\n",
    "    retriever=cc_retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    verbose=True,\n",
    "    return_source_documents=True\n",
    ")\n",
    "# RetrievalQA.from_chain_type:\n",
    "#  • pulls context via cc_retriever\n",
    "#  • concatenates all returned chunks (“stuff”)\n",
    "#  • sends the combined prompt + question to the LLM\n",
    "#  • returns the LLM’s answer (and, if requested, the source documents)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "question = \"What investment strategies does Stock-Watson recommend for inflation hedging?\"\n",
    "result = qa_hf_rerank({\"query\": question})\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"Sources:\", [doc.metadata.get(\"source\") for doc in result[\"source_documents\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8de0ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down the assumptions of linear regression and how to test them.  Understanding these assumptions is *critical* because violating them can lead to inaccurate predictions, misleading conclusions, and generally unreliable models.  I'll be detailed, covering the assumptions, why they matter, and practical tests.\n",
       "\n",
       "**1. Linearity:**\n",
       "\n",
       "* **What it means:**  The relationship between the independent variable(s) (X) and the dependent variable (Y) is linear.  This means when you plot X against Y, the points should roughly form a straight line. It's not about perfect linearity (some \"wiggle room\" is expected), but a *general* linear trend.  It also implies that the relationship is linear at *all* levels of X.\n",
       "* **Why it matters:**  If the relationship is non-linear (e.g., exponential, quadratic, etc.), a linear model will be a poor fit.  It will under- or over-predict certain areas of the data, leading to a high residual error.\n",
       "* **How to Test:**\n",
       "    * **Scatter Plots:** The most basic and visual way.  Plot Y vs. X.  Look for curves, bends, or other non-linear patterns.  It's easy to spot major deviations.\n",
       "    * **Residual Plots:** *This is the most important test.*  Plot the residuals (the difference between the actual Y and the predicted Y) against the predicted Y values (Ŷ) or against the independent variables (X).\n",
       "        * **Linearity Test for One Predictor:** Plot residuals versus one predictor at a time. A random scattering of points around zero indicates linearity. A pattern (e.g., a curve, a funnel shape, a fan shape) suggests non-linearity.\n",
       "        * **Adding Squared Terms:** If a residual plot shows a curve, consider adding a squared term (X²) to your model.  This can help capture a curvilinear relationship.  If the curved pattern disappears in the residual plot, it supports the transformation.\n",
       "    * **Transformation:** Apply transformations to the variables (e.g., log transformation, square root transformation) to linearize the relationship. Then re-run the regression and check the residuals.\n",
       "\n",
       "**2. Independence of Errors (Residuals):**\n",
       "\n",
       "* **What it means:** The error terms (residuals) for each observation are independent of each other.  One error doesn't \"influence\" another.  This is particularly important in time series data or data with a spatial component.  Autocorrelation (correlation between errors) violates this assumption.\n",
       "* **Why it matters:**  Autocorrelation leads to underestimated standard errors.  This means your p-values will be artificially low, and you're more likely to incorrectly reject the null hypothesis (saying a variable is significant when it isn't).\n",
       "* **How to Test:**\n",
       "    * **Durbin-Watson Test:**  A common statistical test that produces a value between 0 and 4.\n",
       "        * 2 ≈ No autocorrelation (ideal)\n",
       "        * Close to 0: Positive autocorrelation (errors are positively correlated with previous errors)\n",
       "        * Close to 4: Negative autocorrelation (errors are negatively correlated with previous errors)\n",
       "        * *Important:* The Durbin-Watson test can be misleading with multiple independent variables.\n",
       "    * **Plot Residuals vs. Order of Observation:**  If you have time series data, plot the residuals against the order of the observations.  A pattern (e.g., alternating positive and negative residuals) indicates autocorrelation.\n",
       "    * **Correlogram (ACF/PACF):** For time series, a correlogram plots the autocorrelation function (ACF) and partial autocorrelation function (PACF). These can help you determine the order of autoregressive models that might be needed to address autocorrelation.\n",
       "\n",
       "**3. Homoscedasticity (Constant Variance of Errors):**\n",
       "\n",
       "* **What it means:** The variance of the error terms is constant across all levels of the independent variables.  In simpler terms, the \"spread\" of the residuals is roughly the same for all predicted values.\n",
       "* **Why it matters:**  Heteroscedasticity (non-constant variance) leads to inefficient estimates of the standard errors. Your confidence intervals will be too narrow or too wide, and your hypothesis tests will be unreliable.\n",
       "* **How to Test:**\n",
       "    * **Residual Plots:**  Again, vital.  Plot residuals vs. predicted Y values (Ŷ) or vs. each independent variable.  Look for a \"fan\" shape or a funnel shape.  This indicates that the variance of the residuals is changing with the level of the predictor.\n",
       "    * **Breusch-Pagan Test / White Test:**  These are formal statistical tests for heteroscedasticity.  They involve regressing the squared residuals on the independent variables.  A significant p-value indicates heteroscedasticity.\n",
       "    * **Spread-Level Plot:** Plot the absolute values of residuals against the predicted values.\n",
       "\n",
       "**4. Normality of Errors:**\n",
       "\n",
       "* **What it means:**  The error terms are normally distributed.  This assumption is less critical than the others, especially with larger sample sizes (due to the Central Limit Theorem).\n",
       "* **Why it matters:**  While not as critical, violated normality can affect the accuracy of p-values and confidence intervals, especially with smaller samples.\n",
       "* **How to Test:**\n",
       "    * **Histogram / Q-Q Plot:**  Plot a histogram of the residuals or use a quantile-quantile (Q-Q) plot.  A Q-Q plot compares the quantiles of the residuals to the quantiles of a normal distribution.  If the residuals are normally distributed, the points on the Q-Q plot should fall close to a straight line.\n",
       "    * **Shapiro-Wilk Test / Kolmogorov-Smirnov Test:**  These are formal statistical tests for normality. A significant p-value indicates non-normality. *Be careful with these tests*, as they can be overly sensitive to minor deviations from normality, especially with large samples.\n",
       "\n",
       "**5. No or Little Multicollinearity (for multiple regression):**\n",
       "\n",
       "* **What it means:**  The independent variables are not highly correlated with each other.\n",
       "* **Why it matters:**  High multicollinearity makes it difficult to determine the individual effect of each predictor variable. It can lead to unstable coefficient estimates (small changes in the data can lead to large changes in the coefficients), inflated standard errors, and unreliable hypothesis tests.\n",
       "* **How to Test:**\n",
       "    * **Correlation Matrix:** Calculate the correlation matrix of the independent variables.  Values close to +1 or -1 indicate high correlation.\n",
       "    * **Variance Inflation Factor (VIF):**  A VIF measures how much the variance of a coefficient estimate is inflated due to multicollinearity. A VIF greater than 10 is generally considered a cause for concern.\n",
       "    * **Tolerance:**  Tolerance is the reciprocal of VIF (Tolerance = 1/VIF).  Values close to 0 indicate high multicollinearity.\n",
       "\n",
       "\n",
       "\n",
       "**Important Considerations and Remedial Actions:**\n",
       "\n",
       "* **Not all assumptions need to be perfectly met:**  Linear regression is *robust* to some violations, especially with larger sample sizes.\n",
       "* **Addressing Violations:**\n",
       "    * **Non-Linearity:** Transformations (log, square root), adding polynomial terms, or using a non-linear model.\n",
       "    * **Autocorrelation:** Include lagged variables, use time series models (ARIMA).\n",
       "    * **Heteroscedasticity:** Use weighted least squares, transform variables, or use robust standard errors.\n",
       "    * **Non-Normality:**  Consider transformations, use non-parametric methods.\n",
       "    * **Multicollinearity:** Remove one of the correlated variables, combine them into a single variable, or use regularization techniques.\n",
       "\n",
       "**Tools for Testing Assumptions:**\n",
       "\n",
       "* **Statistical Software:**  R, Python (with libraries like `statsmodels` and `scikit-learn`), SPSS, Stata.  These packages have built-in functions for many of the tests mentioned above.\n",
       "* **Excel:** Can be used for simple scatter plots and correlation calculations, but less suitable for formal hypothesis tests.\n",
       "\n",
       "To help me tailor my advice further, could you tell me:\n",
       "\n",
       "*   Are you working with a specific dataset or problem?\n",
       "*   What statistical software are you using?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = qa_hybrid.invoke(\"What are the assumptions of Linear Regression and how to test them? Explain each point in detail.\")\n",
    "Markdown(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce248a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccf2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db373363",
   "metadata": {},
   "source": [
    "## ReRanking with CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38d58bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13297/1865039683.py:59: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
      "  reranker = CohereRerank(top_n=10, model=\"rerank-english-v2.0\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import cohere python package. Please install it with `pip install cohere`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/langchain/retrievers/document_compressors/cohere_rerank.py:45\u001b[39m, in \u001b[36mCohereRerank.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcohere\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cohere'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     42\u001b[39m qa_hybrid = RetrievalQA.from_chain_type(\n\u001b[32m     43\u001b[39m     llm=llm,\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# We pass our Ollama-based LLM under the 'llm' parameter. :contentReference[oaicite:5]{index=5}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# We ask the chain to return which source documents were used. :contentReference[oaicite:9]{index=9}\u001b[39;00m\n\u001b[32m     57\u001b[39m )\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Base retriever: hybrid_retriever\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m reranker = \u001b[43mCohereRerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrerank-english-v2.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m cc_retriever = ContextualCompressionRetriever(\n\u001b[32m     61\u001b[39m     base_retriever=qa_hybrid,\n\u001b[32m     62\u001b[39m     base_compressor=reranker\n\u001b[32m     63\u001b[39m )\n\u001b[32m     65\u001b[39m qa_rerank = RetrievalQA(\n\u001b[32m     66\u001b[39m     llm=llm,\n\u001b[32m     67\u001b[39m     retriever=cc_retriever\n\u001b[32m     68\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:224\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    223\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/jarvis-offline/.venv/lib/python3.12/site-packages/langchain/retrievers/document_compressors/cohere_rerank.py:47\u001b[39m, in \u001b[36mCohereRerank.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcohere\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import cohere python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install cohere`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m     )\n\u001b[32m     51\u001b[39m cohere_api_key = get_from_dict_or_env(\n\u001b[32m     52\u001b[39m     values, \u001b[33m\"\u001b[39m\u001b[33mcohere_api_key\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCOHERE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m )\n\u001b[32m     54\u001b[39m client_name = values.get(\u001b[33m\"\u001b[39m\u001b[33muser_agent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlangchain\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: Could not import cohere python package. Please install it with `pip install cohere`."
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "# 1. Load the text file into memory as a LangChain Document.\n",
    "loader = TextLoader(\"docs/StockWatson.txt\")  \n",
    "docs = loader.load()  \n",
    "\n",
    "# 2. Split the document into overlapping chunks of up to 1800 characters each.\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1800,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "chunks = splitter.split_documents(docs)  \n",
    "\n",
    "# 3. Build a BM25 retriever for exact keyword matching over the chunks.\n",
    "bm25 = BM25Retriever.from_documents(chunks)  \n",
    "llm = ChatOllama(model=\"gemma3:12b\")\n",
    "\n",
    "# 4. Build a FAISS retriever for semantic similarity over the same chunks.\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")  \n",
    "faiss_index = FAISS.from_documents(chunks, embeddings)  \n",
    "faiss_ret = faiss_index.as_retriever()  \n",
    "\n",
    "# 5. Ensemble the two retrievers using Reciprocal Rank Fusion.\n",
    "ensemble = EnsembleRetriever(retrievers=[bm25, faiss_ret])  \n",
    "# EnsembleRetriever will merge BM25 and FAISS hits into one unified ranking. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "# 6. Initialize the MultiQueryRetriever correctly:\n",
    "multi_q = MultiQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    # The language model used to rewrite queries.\n",
    "    retriever=ensemble,\n",
    "    # A single BaseRetriever (here, our EnsembleRetriever) that will be called for each rewrite.\n",
    "    include_original=True,\n",
    "    # Include the original user query in addition to LLM-generated rewrites.\n",
    ")\n",
    "# Note: There is no `num_queries` parameter. By default, the internal prompt generates three query variants.\n",
    "# If you need a different number of rewrites, supply a custom `prompt` BasePromptTemplate that specifies your desired count.\n",
    "\n",
    "# 7. Build the RetrievalQA chain using the “stuff” combiner.\n",
    "qa_hybrid = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    # We pass our Ollama-based LLM under the 'llm' parameter. :contentReference[oaicite:5]{index=5}\n",
    "\n",
    "    retriever=multi_q,\n",
    "    # We pass our MultiQueryRetriever instance under the 'retriever' parameter. :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "    chain_type=\"stuff\",\n",
    "    # We explicitly choose the \"stuff\" combine strategy (default if omitted). :contentReference[oaicite:7]{index=7}\n",
    "\n",
    "    verbose=True,\n",
    "    # We enable verbose mode so we can inspect the prompt and retrieval details. :contentReference[oaicite:8]{index=8}\n",
    "\n",
    "    return_source_documents=True\n",
    "    # We ask the chain to return which source documents were used. :contentReference[oaicite:9]{index=9}\n",
    ")\n",
    "# Base retriever: hybrid_retriever\n",
    "reranker = CohereRerank(top_n=10, model=\"rerank-english-v2.0\")\n",
    "cc_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=qa_hybrid,\n",
    "    base_compressor=reranker\n",
    ")\n",
    "\n",
    "qa_rerank = RetrievalQA(\n",
    "    llm=llm,\n",
    "    retriever=cc_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2523596",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa_rerank.invoke(\"What are the assumptions of Linear Regression and how to test them? Explain each point in detail.\")\n",
    "Markdown(answer['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
