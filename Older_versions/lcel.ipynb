{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a74109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "# 4. Invoke with a single dict containing \"question\"\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import sys\n",
    "model_name = \"gemma3:12b\"\n",
    "model_name = \"qwen2.5:7b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0995795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down the concept of LangChain and its application to Large Language Models (LLMs) step by step.\n",
       "\n",
       "**What are Large Language Models (LLMs)?**\n",
       "\n",
       "Large Language Models (LLMs) are a type of artificial intelligence model designed to process and understand human language. They are trained on vast amounts of text data, enabling them to learn patterns, relationships, and nuances of language that allow them to generate coherent and context-specific responses.\n",
       "\n",
       "LLMs have become increasingly popular in recent years due to their ability to perform tasks such as question-answering, text summarization, sentiment analysis, and more. However, LLMs also face several limitations, including:\n",
       "\n",
       "* **Scalability**: As the amount of training data increases, so does the computational complexity of the model.\n",
       "* **Data bias**: Models are only as good as their training data, and if the data is biased or incomplete, the model may not perform well on diverse tasks.\n",
       "* **Contextual understanding**: While LLMs can understand language to some extent, they often struggle with contextual understanding, which is critical for tasks like conversation, debate, and creative writing.\n",
       "\n",
       "**What is LangChain?**\n",
       "\n",
       "LangChain is an open-source framework designed to improve the scalability, flexibility, and performance of Large Language Models (LLMs). The name \"LangChain\" refers to a chain or link in a language model, emphasizing the idea of connecting the dots between different components of the model.\n",
       "\n",
       "LangChain was created by researchers at Stanford University, led by Dr. Emily Dinan, to address the limitations mentioned above. By providing a flexible and modular architecture for LLMs, LangChain enables developers to:\n",
       "\n",
       "* **Scale models**: Increase the size and complexity of LLMs without sacrificing performance.\n",
       "* **Improve contextual understanding**: Introduce additional components that enhance the model's ability to understand context and generate more accurate responses.\n",
       "* **Incorporate domain knowledge**: Add custom domain-specific components that can adapt to specific industries or domains.\n",
       "\n",
       "**Key Components of LangChain**\n",
       "\n",
       "LangChain consists of several key components:\n",
       "\n",
       "1. **Task modules**: These are custom-designed components that implement specific tasks, such as question-answering, sentiment analysis, or text classification.\n",
       "2. **Model wrappers**: These are layers that wrap around the LLM to provide additional functionality, such as handling out-of-vocabulary words or improving contextual understanding.\n",
       "3. **Contextualizers**: These components introduce context into the model's input, enabling it to better understand the conversation history and maintain a consistent tone.\n",
       "4. **Inference engines**: These are specialized modules that improve the model's ability to generate coherent responses and adapt to changing contexts.\n",
       "\n",
       "**Benefits of LangChain**\n",
       "\n",
       "By incorporating these components into an LLM, developers can:\n",
       "\n",
       "* **Improve performance**: Increase the accuracy and coherence of the model's responses.\n",
       "* **Enhance contextual understanding**: Better understand the context and nuances of language.\n",
       "* **Increase scalability**: Scale the model without sacrificing performance or reducing its capabilities.\n",
       "* **Introduce domain-specific knowledge**: Adapt the model to specific domains or industries.\n",
       "\n",
       "**Real-World Applications**\n",
       "\n",
       "LangChain has several potential applications:\n",
       "\n",
       "1. **Conversational AI**: Improve conversational interfaces for customer service, chatbots, and virtual assistants.\n",
       "2. **Content generation**: Generate high-quality content for various formats, such as articles, social media posts, or product descriptions.\n",
       "3. **Question-answering**: Enhance the accuracy of question-answering systems for search engines, knowledge graphs, or educational platforms.\n",
       "4. **Sentiment analysis**: Improve sentiment analysis and emotion detection in text data.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "LangChain is an innovative framework designed to improve the scalability, flexibility, and performance of Large Language Models (LLMs). By incorporating custom task modules, model wrappers, contextualizers, and inference engines, developers can enhance the accuracy and coherence of LLMs. As AI continues to evolve, LangChain has the potential to revolutionize various applications, from conversational interfaces to content generation and question-answering systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=model_name)\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "Markdown(chain.invoke({\"question\": \"What is LangChain for LLMs? Answer in 1000 words.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43506ad",
   "metadata": {},
   "source": [
    "### Basic LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e316ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The translation of \"I love you\" in French depends on the formality and level of affection. Here are a few options:\\n\\n- Je t\\'aime (informal, very intimate)\\n- J\\'adore (more formal, but still affectionate)\\n- Tu m\\'aimes aussi (formal, but with a more casual tone)\\n- Je t\\'aime beaucoup (very formal, expressing strong love)\\n\\nFor an even more romantic touch, you could say:\\n\\n- \"Je t\\'aime plus que tout le monde\" (I love you more than anyone else)\\n- \"Tu es mon tout\" (You are my everything)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Translate to French: {text}\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=model_name)\n",
    "\n",
    "# Output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# LCEL chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"text\": \"I love you\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b061939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Je t\\'aime\" (pronounced \"zhuh tehm\") is a more common way to say it in French, but the translation of \"I love you\" can also be:\\n\\n* \"Je t\\'adore\" (pronounced \"zhuh teh-DOHR\") - a more intense and passionate expression\\n* \"Je t\\'aime beaucoup\" (pronounced \"zhuh teh-mee BOH-keu\") - meaning \"I love you very much\"\\n* \"Je vous aime\" (pronounced \"zhuh voo eh-MAY\") - meaning \"I love you\" (with a more formal tone)\\n\\nHowever, the most common and widely used expression in French is indeed \"Je t\\'aime\".', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-06-22T15:53:10.408138785Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1821306680, 'load_duration': 14014614, 'prompt_eval_count': 32, 'prompt_eval_duration': 16544062, 'eval_count': 151, 'eval_duration': 1790325153}, id='run--1aed982c-5e1e-4679-8da8-43304b9ebc9e-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prompt | llm).invoke({\"text\": \"I love you\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fee61",
   "metadata": {},
   "source": [
    "#### Custom Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd45c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The translation to French would be:\n",
       "\n",
       "LES OMELETTES SONT INCROYABLES"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def capitalize(input: str) -> str:\n",
    "    return {'text': input['text'].upper()}\n",
    "def capitalize(input: str) -> str:\n",
    "    return input['text'].upper()\n",
    "custom_runnable = RunnableLambda(capitalize)\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Translate to French: {text}\")\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=model_name)\n",
    "chain = custom_runnable | prompt |  llm\n",
    "\n",
    "Markdown(chain.invoke({\"text\": \"omelettes are awesome\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6185a",
   "metadata": {},
   "source": [
    "### Larger chains - length 4 and 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35100976",
   "metadata": {},
   "source": [
    "Save a vector index - from a known book or earnings report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0edbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25365/4009400385.py:14: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model = \"nomic-embed-text\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. Read your .txt file\n",
    "with open(\"docs/StockWatson.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# 2. Split text into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2500*2, chunk_overlap=200)\n",
    "docs = splitter.create_documents([data])\n",
    "\n",
    "# 3. Create embeddings\n",
    "embeddings = OllamaEmbeddings(model = \"nomic-embed-text\")\n",
    "\n",
    "# 4. Build FAISS index\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 5. Save the index locally\n",
    "vectorstore.save_local(\"my_faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ff197",
   "metadata": {},
   "source": [
    "RAG - Chain of length 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16fa5db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Panel or longitudinal data can be handled using various econometric methods designed specifically for such data. Here's an overview of common approaches, along with relevant chapters and concepts from the provided textbook:\n",
       "\n",
       "1. **Fixed Effects Model (Chapter 9)**\n",
       "   - **Introduction**: The fixed effects model is used to control for unobserved heterogeneity that is constant over time but varies across individuals.\n",
       "   - **Key Concepts**:\n",
       "     - **Within Transformation**: Transforming the data by subtracting individual-specific means from each observation.\n",
       "     - **Hausman Test**: Testing whether a random effects model or fixed effects model should be used, based on the presence of serial correlation in the residuals.\n",
       "\n",
       "2. **Random Effects Model (Chapter 9)**\n",
       "   - **Introduction**: The random effects model assumes that unobserved heterogeneity is randomly distributed across individuals.\n",
       "   - **Key Concepts**:\n",
       "     - **Between and Within Variances**: Comparing the variances to decide if a fixed or random effects model should be used.\n",
       "     - **Hausman Test**: Testing for endogeneity, where rejection of the null hypothesis suggests that the fixed effects model is preferred.\n",
       "\n",
       "3. **First Differences Model (Chapter 9)**\n",
       "   - **Introduction**: This approach involves differencing the data to eliminate individual-specific time-invariant unobserved heterogeneity.\n",
       "   - **Key Concepts**:\n",
       "     - **Differences in Differences Estimation**: Often used in program evaluation, where differences are taken between individuals before and after a policy change.\n",
       "\n",
       "4. **Generalized Least Squares (GLS) with Time Series Errors (Chapter 9)**\n",
       "   - **Introduction**: GLS is used when the errors exhibit autocorrelation or heteroskedasticity.\n",
       "   - **Key Concepts**:\n",
       "     - **Autocorrelation and Heteroskedasticity**: Testing for these issues using diagnostic tests like the Durbin-Watson statistic or White test.\n",
       "\n",
       "5. **Dynamic Panel Data Models (Chapter 10)**\n",
       "   - **Introduction**: These models include lagged dependent variables to capture dynamic relationships.\n",
       "   - **Key Concepts**:\n",
       "     - **GMM Estimation**: Generalized Method of Moments, which uses moment conditions to estimate parameters in the presence of endogeneity.\n",
       "\n",
       "6. **Random Effects Probit and Logit Models (Chapter 12)**\n",
       "   - **Introduction**: These models are used when the dependent variable is binary.\n",
       "   - **Key Concepts**:\n",
       "     - **Logit vs. Probit**: Differences between these two models, with logit being more commonly used due to its simplicity.\n",
       "\n",
       "7. **Cluster-Robust Standard Errors (Chapter 14)**\n",
       "   - **Introduction**: This method accounts for clustering in the data.\n",
       "   - **Key Concepts**:\n",
       "     - **Clustered Data**: Situations where observations are grouped into clusters, such as states or firms.\n",
       "\n",
       "8. **Fixed Effects Probit and Logit Models (Chapter 12)**\n",
       "   - **Introduction**: These models control for unobserved heterogeneity that is constant over time.\n",
       "   - **Key Concepts**:\n",
       "     - **Within Transformation**: Similar to the fixed effects model but applied in a binary context.\n",
       "\n",
       "9. **Instrumental Variables with Panel Data (Chapter 10)**\n",
       "   - **Introduction**: This method addresses endogeneity by using instrumental variables specific to panel data.\n",
       "   - **Key Concepts**:\n",
       "     - **Valid Instruments**: Exogenous and relevant instruments, overidentifying restrictions test, weak instrument diagnostics.\n",
       "\n",
       "Each of these methods is crucial for handling the complexities of panel or longitudinal data. The references provided are based on the structure and content of the textbook you've described, with specific chapters highlighted where each method is introduced and explained in detail."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Retriever\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"my_faiss_index\",\n",
    "    OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 2. Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question based on the documents below. Answer in an elaborate manner, listing all the details.\"),\n",
    "    (\"human\", \"Documents:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "\n",
    "# 3. LLM\n",
    "llm = OllamaLLM(model=model_name)\n",
    "\n",
    "# 4. Parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 1. Bundle step: run retriever AND echo the question\n",
    "bundle = RunnableParallel({\n",
    "    \"docs\": retriever,\n",
    "    # pull \"question\" out of the input dict\n",
    "    \"question\": RunnableLambda(lambda inp: inp[\"question\"])\n",
    "})\n",
    "\n",
    "# 2. Assemble into exactly the inputs your prompt needs\n",
    "assemble = RunnableLambda(lambda inp: {\n",
    "    \"context\": \"\\n\".join(d.page_content for d in inp[\"docs\"]),\n",
    "    \"question\": inp[\"question\"]\n",
    "})\n",
    "\n",
    "# 3. Now chain it all\n",
    "chain = bundle | assemble | prompt | llm | parser\n",
    "\n",
    "# 4. Invoke with a single dict containing \"question\"\n",
    "#response = chain.invoke({\"question\": \"What are the ways of handling panel or longitudinal data? Give me a brief introduction to each one of them. Give references chapterwise from the context too\"})\n",
    "#Markdown(response)\n",
    "\n",
    "#for chunk in chain.stream({\"question\": \"What are the ways of handling panel or longitudinal data? Give me a brief introduction to each one of them. Give references chapterwise from the context too\"}):\n",
    "#    display(Markdown(chunk))\n",
    "\n",
    "\n",
    "output = \"\"\n",
    "for chunk in chain.stream({\"question\": \"What are the ways of handling panel or longitudinal data? Give me a brief introduction to each one of them. Give references chapterwise from the context too\"}):\n",
    "    output += chunk\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02f6184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The difference-in-differences (DiD) method is a quasi-experimental approach used in econometrics and social sciences for estimating causal effects by comparing the changes over time between a treatment group and a control group. It addresses specific challenges that arise when conducting impact evaluations or experimental studies, particularly issues related to unobserved heterogeneity and temporal variation.\n",
       "\n",
       "### Origins of Difference-in-Differences\n",
       "The DiD method was developed as an alternative to traditional randomized controlled trials (RCTs) in situations where random assignment is not feasible due to practical constraints. It leverages the natural experiments that occur when a policy or intervention affects one group differently than others over time. The core idea is to compare trends before and after the treatment in both the treated and control groups, using differences between these trends as an estimate of the treatment effect.\n",
       "\n",
       "### How Difference-in-Differences Works\n",
       "The method works by exploiting the variation in the timing of the policy change across different units or regions. It relies on the assumption that, in the absence of the intervention, the treated group would have followed a similar path to the control group had they not been treated. This assumption is known as the \"parallel trends\" assumption.\n",
       "\n",
       "The DiD estimator can be expressed mathematically as follows:\n",
       "\\[ \\text{DiD} = (\\bar{Y}_{Treated, post} - \\bar{Y}_{Control, post}) - (\\bar{Y}_{Treated, pre} - \\bar{Y}_{Control, pre}) \\]\n",
       "\n",
       "Where:\n",
       "- \\(\\bar{Y}_{Treated, post}\\) is the average outcome for the treated group after the policy implementation.\n",
       "- \\(\\bar{Y}_{Control, post}\\) is the average outcome for the control group after the policy implementation.\n",
       "- \\(\\bar{Y}_{Treated, pre}\\) is the average outcome for the treated group before the policy implementation.\n",
       "- \\(\\bar{Y}_{Control, pre}\\) is the average outcome for the control group before the policy implementation.\n",
       "\n",
       "### Assumptions and Challenges\n",
       "For DiD to yield valid results, several key assumptions must hold:\n",
       "1. **Parallel Trends**: In the absence of the treatment, trends in the outcome variables should be parallel between the treated and control groups.\n",
       "2. **No Anticipation Effects**: The effects of the intervention do not start before it is actually implemented.\n",
       "3. **No Reverse Causality**: The policy or intervention does not cause changes in the covariates.\n",
       "\n",
       "### Example from the Text\n",
       "In the context provided, consider a researcher studying the effects of a new fertilizer on crop yields. Suppose the researcher applies four levels of fertilizer to 100 one-acre parcels over four treatment periods:\n",
       "- Treatment Level 1 (no fertilizer)\n",
       "- Treatment Level 2 (50% recommended amount)\n",
       "- Treatment Level 3 (full recommended amount)\n",
       "- Treatment Level 4 (150% recommended amount)\n",
       "\n",
       "To improve the experimental design, a better approach could be to use random assignment of treatments across parcels. This would help ensure that any unobserved heterogeneity is randomly distributed between treated and control groups.\n",
       "\n",
       "However, if randomization is not feasible, DiD can still be applied by comparing yield changes over time in different regions or periods:\n",
       "- One region uses the new fertilizer starting in Year 2.\n",
       "- A similar region continues to use the old method as a control group for comparison.\n",
       "\n",
       "If we observe that treated regions show greater increases in crop yields compared to the untreated regions, after accounting for pre-treatment trends, this difference can be attributed to the new fertilizer. For example:\n",
       "- Before treatment (Year 1), both regions have an average yield of 500 kg/acre.\n",
       "- After implementation (Year 2), the treated region has an average yield of 600 kg/acre.\n",
       "- The control region continues at 500 kg/acre.\n",
       "\n",
       "Using DiD:\n",
       "\\[ \\text{DiD} = (600 - 500) - (500 - 500) = 100 \\, \\text{kg/acre} \\]\n",
       "\n",
       "This estimated increase of 100 kg/acre can be interpreted as the treatment effect of the new fertilizer on crop yields.\n",
       "\n",
       "### Conclusion\n",
       "The DiD method provides a robust way to estimate causal effects when random assignment is not feasible. By comparing trends before and after an intervention between treated and control groups, it helps mitigate issues like unobserved heterogeneity and temporal variation. The approach leverages natural experiments to provide credible estimates of treatment impacts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"Explain in detail the concept of Difference in differences. Start from what problem did it set out to solve and how it solved it. End with atleast one example from the text.\"})\n",
    "Markdown(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
