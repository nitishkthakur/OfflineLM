{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79a74109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9673294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0995795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain is an open-source library developed to enhance the performance and efficiency of Large Language Models (LLMs) in various Natural Language Processing (NLP) tasks. In this response, we'll break down what LangChain is, its key features, and how it can benefit LLMs.\n",
       "\n",
       "**What is LangChain?**\n",
       "\n",
       "LangChain is a Python library designed to facilitate the integration of language models with other tools and services used in NLP, such as data pipelines, databases, and workflow management systems. It provides a standardized interface for interacting with various components of the LLM pipeline, allowing developers to build custom workflows that optimize the performance and efficiency of their LLMs.\n",
       "\n",
       "**Key Features**\n",
       "\n",
       "LangChain is built around several key features that make it an attractive choice for integrating language models with other tools and services:\n",
       "\n",
       "1. **Modular Architecture**: LangChain's modular architecture allows users to choose only the components they need, making it easy to integrate with existing workflows and pipelines.\n",
       "2. **Standardized Interface**: The library provides a standardized interface for interacting with LLMs, databases, and other NLP components, ensuring consistency across different tools and services.\n",
       "3. **Scalability**: LangChain is designed to handle large volumes of data and complex workflows, making it suitable for enterprise-level applications.\n",
       "4. **Flexibility**: Users can easily customize the library to fit their specific use cases, incorporating new features or modifying existing ones.\n",
       "\n",
       "**Components of LangChain**\n",
       "\n",
       "LangChain consists of several components that work together to provide a comprehensive set of tools for integrating language models:\n",
       "\n",
       "1. **Web3**: This component provides an interface to interact with blockchain-based databases and services.\n",
       "2. **Task**: The Task component allows users to define complex workflows, including data processing, model inference, and task execution.\n",
       "3. **Data**: The Data component manages the storage, retrieval, and manipulation of large datasets used by LLMs.\n",
       "4. **Pipeline**: This component enables users to build and manage custom workflows that integrate with their language models.\n",
       "\n",
       "**Benefits for LLMs**\n",
       "\n",
       "LangChain offers several benefits when it comes to Large Language Models:\n",
       "\n",
       "1. **Improved Efficiency**: By integrating LangChain with existing workflows and pipelines, developers can optimize the performance of their LLMs, reducing processing time and increasing throughput.\n",
       "2. **Enhanced Scalability**: The library's modular architecture and scalability features enable users to handle large volumes of data and complex workflows, making it suitable for enterprise-level applications.\n",
       "3. **Increased Flexibility**: LangChain's customizable nature allows users to incorporate new features or modify existing ones, ensuring the library remains relevant as LLMs evolve.\n",
       "\n",
       "**Real-World Applications**\n",
       "\n",
       "LangChain has numerous real-world applications across various industries:\n",
       "\n",
       "1. **Chatbots and Conversational AI**: Integrating LangChain with LLMs can create more efficient and effective chatbots that understand context and respond accordingly.\n",
       "2. **Data Annotation and Processing**: The library's data management features enable fast and accurate annotation, making it suitable for large-scale NLP projects.\n",
       "3. **Content Generation**: By integrating LangChain with LLMs, developers can build content generation tools that produce high-quality text quickly and efficiently.\n",
       "\n",
       "**Future Developments**\n",
       "\n",
       "The development of LangChain is ongoing, with new features and updates being added regularly:\n",
       "\n",
       "1. **Improved Workflow Management**: Upcoming releases will include enhanced workflow management capabilities, allowing users to define more complex workflows and integrate multiple components seamlessly.\n",
       "2. **New Data Sources**: The library may soon support integration with new data sources, such as external databases or cloud-based storage services.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "LangChain is an essential tool for developers working with Large Language Models in NLP applications. Its modular architecture, standardized interface, scalability features, and flexibility make it an attractive choice for integrating language models with other tools and services. By providing a comprehensive set of components, LangChain enables users to build custom workflows that optimize the performance and efficiency of their LLMs. As LLMs continue to evolve, LangChain's continued development will ensure it remains relevant in the NLP landscape."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.2:3b\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "Markdown(chain.invoke({\"question\": \"What is LangChain for LLMs? Answer in 1000 words.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43506ad",
   "metadata": {},
   "source": [
    "### Basic LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e316ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The translation of \"I love you\" in French depends on the formality and level of affection. Here are a few options:\\n\\n- Je t\\'aime (informal, very intimate)\\n- J\\'adore (more formal, but still affectionate)\\n- Tu m\\'aimes aussi (formal, but with a more casual tone)\\n- Je t\\'aime beaucoup (very formal, expressing strong love)\\n\\nFor an even more romantic touch, you could say:\\n\\n- \"Je t\\'aime plus que tout le monde\" (I love you more than anyone else)\\n- \"Tu es mon tout\" (You are my everything)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Translate to French: {text}\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "\n",
    "# Output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# LCEL chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"text\": \"I love you\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b061939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Je t\\'aime\" (pronounced \"zhuh tehm\") is a more common way to say it in French, but the translation of \"I love you\" can also be:\\n\\n* \"Je t\\'adore\" (pronounced \"zhuh teh-DOHR\") - a more intense and passionate expression\\n* \"Je t\\'aime beaucoup\" (pronounced \"zhuh teh-mee BOH-keu\") - meaning \"I love you very much\"\\n* \"Je vous aime\" (pronounced \"zhuh voo eh-MAY\") - meaning \"I love you\" (with a more formal tone)\\n\\nHowever, the most common and widely used expression in French is indeed \"Je t\\'aime\".', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-06-22T15:53:10.408138785Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1821306680, 'load_duration': 14014614, 'prompt_eval_count': 32, 'prompt_eval_duration': 16544062, 'eval_count': 151, 'eval_duration': 1790325153}, id='run--1aed982c-5e1e-4679-8da8-43304b9ebc9e-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prompt | llm).invoke({\"text\": \"I love you\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fee61",
   "metadata": {},
   "source": [
    "#### Custom Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eecd45c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The translation to French would be:\n",
       "\n",
       "LES OMELETTES SONT INCROYABLES"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def capitalize(input: str) -> str:\n",
    "    return {'text': input['text'].upper()}\n",
    "def capitalize(input: str) -> str:\n",
    "    return input['text'].upper()\n",
    "custom_runnable = RunnableLambda(capitalize)\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Translate to French: {text}\")\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"llama3.2:3b\")\n",
    "chain = custom_runnable | prompt |  llm\n",
    "\n",
    "Markdown(chain.invoke({\"text\": \"omelettes are awesome\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6185a",
   "metadata": {},
   "source": [
    "### Larger chains - length 4 and 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35100976",
   "metadata": {},
   "source": [
    "Save a vector index - from a known book or earnings report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e0edbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "# 1. Read your .txt file\n",
    "with open(\"docs/StockWatson.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# 2. Split text into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2500*2, chunk_overlap=200)\n",
    "docs = splitter.create_documents([data])\n",
    "\n",
    "# 3. Create embeddings\n",
    "embeddings = OllamaEmbeddings(model = \"nomic-embed-text\")\n",
    "\n",
    "# 4. Build FAISS index\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 5. Save the index locally\n",
    "vectorstore.save_local(\"my_faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ff197",
   "metadata": {},
   "source": [
    "RAG - Chain of length 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16fa5db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Handling panel or longitudinal data involves several methods, each with its own advantages and appropriate contexts. Here's an overview of some common approaches:\n",
       "\n",
       "1. **Fixed Effects Model**:\n",
       "   - **Description**: This model controls for unobserved heterogeneity that is constant over time but varies across entities (e.g., individuals, firms). It does this by including entity-specific intercepts.\n",
       "   - **Use Case**: Suitable when the unobserved effects are correlated with the independent variables of interest. For example, if you're analyzing the impact of a policy on different states and want to control for state-specific characteristics that don't change over time.\n",
       "   - **Advantages**: It can address endogeneity issues by isolating within-entity variation.\n",
       "   - **Disadvantages**: Can be less efficient when the number of entities is large compared to the number of time periods.\n",
       "\n",
       "2. **Random Effects Model**:\n",
       "   - **Description**: This model assumes that unobserved heterogeneity is not correlated with the independent variables and can vary randomly across entities over time. It includes both entity-specific intercepts (as in fixed effects) but treats them as random draws from a distribution.\n",
       "   - **Use Case**: Appropriate when you believe that the omitted variables are not related to the regressors of interest, or when there's too much variation among entities and time periods for fixed effects.\n",
       "   - **Advantages**: More efficient than fixed effects if the unobserved heterogeneity is truly random.\n",
       "   - **Disadvantages**: If the unobserved heterogeneity is correlated with the independent variables, this model can produce biased estimates.\n",
       "\n",
       "3. **Pooled Ordinary Least Squares (OLS)**:\n",
       "   - **Description**: This method assumes that there's no unobserved heterogeneity across entities or over time. It treats all data as if they were cross-sectional.\n",
       "   - **Use Case**: Suitable when the assumption of no omitted variables is likely to hold, and you have a large number of time periods relative to the number of entities.\n",
       "   - **Advantages**: Simple and computationally efficient.\n",
       "   - **Disadvantages**: Can be biased in the presence of unobserved heterogeneity.\n",
       "\n",
       "4. **Randomized Controlled Trials (RCTs) or Quasi-Experiments**:\n",
       "   - **Description**: These involve either randomized assignment to treatment groups or natural experiments where a policy change occurs randomly across some units.\n",
       "   - **Use Case**: To estimate causal effects in the presence of unobserved heterogeneity, by comparing outcomes before and after a treatment or between treated and control groups.\n",
       "   - **Advantages**: Can provide unbiased estimates if proper randomization is achieved (RCTs) or if the natural experiment meets certain assumptions.\n",
       "   - **Disadvantages**: May require strong assumptions about the comparability of units.\n",
       "\n",
       "5. **Difference-in-Differences (DiD)**:\n",
       "   - **Description**: This method compares changes in outcomes over time between a treatment group and a control group to estimate the effect of an intervention.\n",
       "   - **Use Case**: Often used when you have panel data and can identify a \"treatment\" and \"control\" group, where the treatment group experiences the intervention while the control group does not.\n",
       "   - **Advantages**: Can handle both within- and between-entity variation.\n",
       "   - **Disadvantages**: Requires that the pre-treatment trends in outcomes are similar across groups.\n",
       "\n",
       "6. **Generalized Method of Moments (GMM)**:\n",
       "   - **Description**: This is a more general approach used when there are multiple moment conditions available, allowing for estimation even if some of them are not identified.\n",
       "   - **Use Case**: Often used with panel data to address issues like serial correlation and heteroskedasticity.\n",
       "   - **Advantages**: Flexible and can handle complex models with many instruments.\n",
       "   - **Disadvantages**: Can be computationally intensive.\n",
       "\n",
       "7. **Dynamic Panel Data Models**:\n",
       "   - **Description**: These models account for the lagged dependent variable, capturing how past values of the dependent variable affect current outcomes.\n",
       "   - **Use Case**: Suitable when there is a clear temporal dynamic in the data, such as stock prices or economic growth over time.\n",
       "   - **Advantages**: Captures both short-term and long-term effects.\n",
       "   - **Disadvantages**: Can be challenging to estimate due to potential autocorrelation issues.\n",
       "\n",
       "8. **Mundlak’s Random Effects Model**:\n",
       "   - **Description**: This model includes the expected value of the unobserved entity-specific effect in the regression equation, allowing for a more flexible specification.\n",
       "   - **Use Case**: Useful when you have some information about the factors causing unobserved heterogeneity.\n",
       "   - **Advantages**: Can handle both fixed and random effects simultaneously.\n",
       "   - **Disadvantages**: Requires additional data or assumptions.\n",
       "\n",
       "9. **Panel Probit Models**:\n",
       "   - **Description**: An extension of probit models to panel data, used when the dependent variable is binary.\n",
       "   - **Use Case**: Suitable for analyzing discrete outcomes in a panel setting.\n",
       "   - **Advantages**: Accounts for unobserved heterogeneity and time-invariant characteristics.\n",
       "   - **Disadvantages**: Computationally intensive.\n",
       "\n",
       "10. **Heterogeneous Effects Models**:\n",
       "    - **Description**: These models allow for different effects of the independent variables on different entities or at different times.\n",
       "    - **Use Case**: When you suspect that the relationship between variables might differ across subgroups or time periods.\n",
       "    - **Advantages**: More flexible and can capture heterogeneous responses to treatments or policies.\n",
       "    - **Disadvantages**: Can be complex to estimate and interpret.\n",
       "\n",
       "These methods provide a comprehensive toolkit for analyzing panel data, allowing researchers to address various issues such as omitted variable bias, unobserved heterogeneity, and dynamic effects."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Retriever\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"my_faiss_index\",\n",
    "    OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 2. Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question based on the documents below. Answer in an elaborate manner, listing all the details.\"),\n",
    "    (\"human\", \"Documents:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "\n",
    "# 3. LLM\n",
    "llm = OllamaLLM(model=\"qwen2.5:7b\")\n",
    "\n",
    "# 4. Parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 1. Bundle step: run retriever AND echo the question\n",
    "bundle = RunnableParallel({\n",
    "    \"docs\": retriever,\n",
    "    # pull \"question\" out of the input dict\n",
    "    \"question\": RunnableLambda(lambda inp: inp[\"question\"])\n",
    "})\n",
    "\n",
    "# 2. Assemble into exactly the inputs your prompt needs\n",
    "assemble = RunnableLambda(lambda inp: {\n",
    "    \"context\": \"\\n\".join(d.page_content for d in inp[\"docs\"]),\n",
    "    \"question\": inp[\"question\"]\n",
    "})\n",
    "\n",
    "# 3. Now chain it all\n",
    "chain = bundle | assemble | prompt | llm | parser\n",
    "\n",
    "# 4. Invoke with a single dict containing \"question\"\n",
    "response = chain.invoke({\"question\": \"What are the ways of handling panel or longitudinal data? Give me a brief introduction to each one of them\"})\n",
    "Markdown(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
